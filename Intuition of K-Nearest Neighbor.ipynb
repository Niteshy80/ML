{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "K-nearest neighbors (KNN) is a type of supervised learning algorithm which is used for both regression and classification purposes, but mostly it is used for the later. Given a dataset with different classes, KNN tries to predict the correct class of test data by calculating the distance between the test data and all the training points. It then selects the k points which are closest to the test data.  Once the points are selected, the algorithm calculates the probability (in case of classification) of the test point belonging to the classes of the k training points and the class with the highest probability is selected. In the case of a regression problem, the predicted value is the mean of the k selected training points.\n",
    "\n",
    "\n",
    "Let’s understand this with an illustration:\n",
    "\n",
    "\n",
    "1)\tGiven a training dataset as given below. We have a new test data that we need to assign to one of the two classes.\n",
    "\n",
    "<img src=\"1.png\" width=\"\">\n",
    "                                      \n",
    "\n",
    "2)\tNow, the k-NN algorithm calculates the distance between the test data and the given training data.\n",
    "\n",
    "<img src=\"2.png\" width=\"\">\n",
    "\n",
    "                                                        \n",
    "3)\tAfter calculating the distance, it will select the k training points which are nearest to the test data. Let’s assume the value of k is 3 for our example.\n",
    "\n",
    "<img src=\"3.png\" width=\"\">                                            \n",
    "\n",
    "\n",
    "4)\tNow, 3 nearest neighbors are selected, as shown in the figure above. Let’s see in which class our test data will be assigned :\n",
    "\n",
    "Number of Green class values = 2\n",
    "Number of Red class values = 1\n",
    "Probability(Green) = 2/3\n",
    "Probability(Red) = 1/3\n",
    "\n",
    "Since the probability for Green class is higher than Red, the k-NN algorithm will assign the test data to the Green class.\n",
    "\n",
    "Similarly, if this were the case of a regression problem, the predicted value for the test data will simply be the mean of all the 3 nearest values.\n",
    "\n",
    "This is the basic working algorithm for k-NN. Let’s understand how the distance is calculated :\n",
    "\n",
    "### Euclidean Distance: \n",
    "\n",
    "It is the most commonly used method to calculate the distance between two points.\n",
    "The Euclidean distance between two points ‘p(p1,p2)’ and ‘q(q1,q2)’ is calculated  as :\n",
    "\n",
    "<img src=\"4.png\" width=\"\">       image source : Wikipedia\n",
    "\n",
    "<img src=\"5.png\" width=\"\">\n",
    "\n",
    "                                          \n",
    "Similarly,for n-dimensional space, the Euclidean distance is given as :\n",
    "\n",
    "<img src=\"6.png\" width=\"\">\n",
    " \n",
    "### Hamming distance\n",
    "A/c to Wikipedia, hamming distance is a distance metric that measures the number of mismatches between two vectors. It is mostly used in the case of categorical data.\n",
    "\n",
    "<img src=\"7.png\" width=\"\">\n",
    "                                                               \n",
    "Generally, if we have features as categorical data then we consider the difference to be 0 if both the values are the same and the difference is 1 if both the values are different.\n",
    "\n",
    "### Manhattan Distance\n",
    "A/c to Wikipedia, The Manhattan distance, also known as L1 norm, Taxicab norm, Rectilinear distance or City block distance. This distance represents the sum of the absolute differences between the opposite values in vectors.\n",
    "\n",
    "<img src=\"8.png\" width=\"\">\n",
    "                                                           \n",
    "Manhattan Distance is less influenced by outliers than the Euclidean distance. With very high dimensional data it is more preferred. \n",
    "\n",
    "### Lazy Learners\n",
    "\n",
    "k-NN algorithms are often termed as Lazy learners. Let’s understand why is that. Most of the algorithms like Bayesian classification, logistic regression, SVM etc., are called Eager learners. These algorithms generalize over the training set before receiving the test data i.e. they create a model based on the training data before receiving the test data and then do the prediction/classification on the test data.\n",
    "But this is not the case with the k-NN algorithm. It doesn’t create a generalized model for the training set but waits for the test data. Once test data is provided then only it starts generalizing the training data to classify the test data.  So, a lazy learner just stores the training data and waits for the test set. Such algorithms work less while training and more while classifying a given test dataset.\n",
    "\n",
    "### Weighted Nearest Neighbor\n",
    "In weighted k-NN, we assign weights to the k nearest neighbors.The weights are typically assigned on the basis of distance.\n",
    "Sometimes rest of data points are assigned a weight of 0 also. The main intuition is that the points in neighbor should have more weights than father points.\n",
    "\n",
    "\n",
    "\n",
    "### Choosing the value of k\n",
    "The value of k affects the k-NN classifier drastically. The flexibility of the model decreases with the increase of ‘k’.  With lower value of ‘k’ variance is high and bias is low but as we increase the value of ‘k’ variance starts decreasing and bias starts increasing. With very low values of ‘k’ there is a chance of algorithm overfitting the data whereas with very high value of ‘k’ there is a chance of underfitting. \n",
    "Let’s visualize the trade-off between ‘1/k’, train error rate and test error rate:\n",
    "\n",
    "<img src=\"9.png\" width=\"\">  image source: “ISLR”\n",
    "\n",
    "We can clearly see that the train error rate increases with the increase in the value of ‘k’ whereas test error rate decreases initially and then increases again.  So, our goal should be to choose such value of ‘k’ for which we get a minimum of both the errors and avoid overfitting as well as underfitting.\n",
    "We use different ways to calculate the optimum value of ‘k’ such as cross validation, error versus k curve, checking accuracy for each value of ‘k’ etc. \n",
    "\n",
    "### Pros and Cons of k-NN Algorithm\n",
    "\n",
    "Pros:\n",
    "*\tIt can be used for both regression and classification problems.\n",
    "*\tIt is very simple and easy to implement.\n",
    "*\tMathematics behind the algorithm is easy to understand.\n",
    "*\tThere is no need to create model or do hyperparameter tuning.\n",
    "*   KNN doesn't make any assumption for the distribution of the given data.\n",
    "*   There is not much time cost in training phase.\n",
    "\n",
    "Cons:\n",
    "*\tFinding the optimum value of ‘k’\n",
    "*\tIt takes a lot of time to compute the distance between each test sample and all training samples.\n",
    "*\tSince the model is not saved beforehand in this algorithm (lazy learner), so every time one predicts a test value, it follows the same steps again and again. \n",
    "*\tSince, we need to store the whole training set for every test set, it requires a lot of space.\n",
    "*\tIt is not suitable for high dimensional data.\n",
    "*   Expensive in testing phase\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
